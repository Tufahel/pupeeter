const puppeteer = require('puppeteer');
const fs = require('fs');
const path = require('path');
const ScrapedJobsDB = require('../utils/ScrapedJobsDB');

/**
 * REGULAR JOBS SCRAPER - Only fetches non-sponsored jobs
 * This scraper identifies and skips sponsored jobs, focusing on regular jobs with posting dates
 */

// Simple CSV export function
function saveJobsToCSV(jobs, filename) {
  try {
    const exportsDir = path.join(__dirname, '..', 'exports');
    if (!fs.existsSync(exportsDir)) {
      fs.mkdirSync(exportsDir, { recursive: true });
    }
    
    const csvHeaders = [
      'Job Title', 'Salary', 'Contact', 'Email', 'Location', 'Posting ID', 
      'Posted Date', 'Category', 'Employment Type', 'Company', 'Requirements', 
      'Benefits', 'Description', 'Job URL', 'Scraped At', 'Is Sponsored'
    ];
    
    const csvRows = jobs.map(job => [
      job.title || '',
      job.salary || '',
      job.contact || '',
      job.email || '',
      job.location || '',
      job.posting_id || '',
      job.posted_date || '',
      job.category || '',
      job.employment_type || '',
      job.company || '',
      (job.requirements || '').replace(/"/g, '""').substring(0, 500),
      (job.benefits || '').replace(/"/g, '""').substring(0, 300),
      (job.description || '').replace(/"/g, '""').substring(0, 1000),
      job.url || '',
      job.scraped_at || new Date().toISOString(),
      job.is_sponsored || 'No'
    ]);
    
    const csvContent = [
      csvHeaders.join(','),
      ...csvRows.map(row => row.map(field => `"${field}"`).join(','))
    ].join('\n');
    
    const filePath = path.join(exportsDir, filename);
    fs.writeFileSync(filePath, csvContent, 'utf8');
    
    const stats = fs.statSync(filePath);
    const fileSizeInMB = (stats.size / (1024 * 1024)).toFixed(2);
    
    return {
      success: true,
      filename,
      filePath,
      recordCount: jobs.length,
      fileSize: `${fileSizeInMB} MB`
    };
    
  } catch (error) {
    return {
      success: false,
      error: error.message
    };
  }
}

// Job extraction function (enhanced version)
async function extractJobFromUrl(page, jobUrl) {
  try {
    console.log(`üîç Extracting from: ${jobUrl}`);
    
    await page.goto(jobUrl, { waitUntil: 'networkidle0', timeout: 30000 });
    await new Promise(resolve => setTimeout(resolve, 3000));
    
    const jobDetails = await page.evaluate(() => {
      const bodyText = document.body.textContent || '';
      
      // Skip if showing challenge
      if (bodyText.toLowerCase().includes('verify you are human') || 
          bodyText.toLowerCase().includes('checking your browser')) {
        return null;
      }
      
      const jobData = {
        title: '',
        salary: '',
        contact: '',
        email: '',
        location: '',
        posting_id: '',
        posted_date: '',
        category: '',
        employment_type: '',
        requirements: '',
        description: '',
        company: '',
        benefits: '',
        is_sponsored: 'No'
      };
      
      // Title
      let title = document.title || '';
      title = title.replace(/\s*-\s*expatriates\.com.*$/i, '').trim();
      title = title.replace(/^.*?Jobs,\s*/, '').replace(/,\s*\d+$/, '').trim();
      if (title) jobData.title = title;
      
      // Salary patterns
      const salaryPatterns = [
        bodyText.match(/Salary:?\s*(\d{3,5})/i),
        bodyText.match(/salary\s+(\d{3,5})/i),
        bodyText.match(/(\d{4})\s*(?:SR|SAR|Riyal)/i),
        bodyText.match(/(\d{3,5})\s*SAR/i)
      ];
      
      for (const match of salaryPatterns) {
        if (match && match[1]) {
          jobData.salary = match[1];
          break;
        }
      }
      
      // Contact numbers
      const contacts = [];
      const phonePatterns = [
        bodyText.match(/\b(05\d{8})\b/g),
        bodyText.match(/\b(01\d{8})\b/g),
        bodyText.match(/\+(966\d{9})/g),
        bodyText.match(/\b(\d{10})\b/g)
      ];
      
      phonePatterns.forEach(matches => {
        if (matches) {
          matches.forEach(phone => {
            if (phone && phone.length >= 9) {
              contacts.push(phone);
            }
          });
        }
      });
      
      if (contacts.length > 0) {
        jobData.contact = [...new Set(contacts)].join(', ');
      }
      
      // Email
      const emailMatches = [
        bodyText.match(/From:\s*([^\s@]+@[^\s\n\r]+\.[^\s\n\r]+)/i),
        bodyText.match(/([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,})/),
        bodyText.match(/Email:\s*([^\s@]+@[^\s\n\r]+)/i)
      ];
      
      for (const match of emailMatches) {
        if (match && match[1] && !match[1].includes('expatriates.com')) {
          jobData.email = match[1];
          break;
        }
      }
      
      // Location
      const locationMatches = [
        bodyText.match(/Region:\s*([^\n\r()]+)/i),
        bodyText.match(/Location:\s*\[([^\]]+)\]/i),
        bodyText.match(/Location:\s*([^\n\r]+)/i),
        bodyText.match(/City:\s*([^\n\r]+)/i)
      ];
      
      for (const match of locationMatches) {
        if (match && match[1]) {
          jobData.location = match[1].trim();
          break;
        }
      }
      
      // Posted date
      const dateMatches = [
        bodyText.match(/Posted:\s*([^\n\r]+)/i),
        bodyText.match(/Date:\s*([^\n\r]+)/i),
        bodyText.match(/(\d{1,2}\/\d{1,2}\/\d{4})/),
        bodyText.match(/(\d{1,2}-\d{1,2}-\d{4})/),
        bodyText.match(/(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\s+\d{1,2},?\s+\d{4}/i)
      ];
      
      for (const match of dateMatches) {
        if (match && match[1]) {
          jobData.posted_date = match[1].trim();
          break;
        }
      }
      
      // Description - Get everything after WhatsApp
      const whatsappIndex = bodyText.indexOf('Chat on WhatsApp');
      if (whatsappIndex > -1) {
        let afterWhatsapp = bodyText.substring(whatsappIndex + 20);
        
        const endMarkers = [
          'Back', 'Next', 'Email to a Friend', 'Page View Count', 
          '¬© 2025', 'NEVER PAY ANY KIND', 'Facebook', 'Twitter'
        ];
        
        let endIndex = afterWhatsapp.length;
        for (const marker of endMarkers) {
          const markerIndex = afterWhatsapp.indexOf(marker);
          if (markerIndex > -1 && markerIndex < endIndex) {
            endIndex = markerIndex;
          }
        }
        
        let description = afterWhatsapp.substring(0, endIndex).trim();
        
        if (description) {
          description = description.replace(/[ \t]+/g, ' ').replace(/\n\s*\n\s*\n/g, '\n\n').trim();
          description = description
            .replace(/Job Details:\s*/gi, '\nüìã Job Details:\n')
            .replace(/Requirements:\s*/gi, '\n‚úÖ Requirements:\n')
            .replace(/What We Offer:\s*/gi, '\nüéÅ What We Offer:\n');
          
          if (description.length > 1000) {
            description = description.substring(0, 1000) + '...';
          }
          
          jobData.description = description;
        }
      }
      
      return jobData;
    });
    
    if (jobDetails) {
      jobDetails.url = jobUrl;
      jobDetails.scraped_at = new Date().toISOString();
      return jobDetails;
    }
    
    return null;
    
  } catch (error) {
    console.log(`‚ùå Error extracting from ${jobUrl}:`, error.message);
    return null;
  }
}

module.exports = async (req, res) => {
  let browser;
  
  try {
    console.log('üöÄ Starting OPTIMIZED REGULAR JOBS scraper (skipping sponsored)...');
    
    // Initialize scraped jobs database
    const scrapedDB = new ScrapedJobsDB();
    
    // Parse query parameters
    const maxJobs = parseInt(req.query.maxJobs) || 999999;
    const quickMode = req.query.quick === 'true';
    const isAutoMode = req.query.auto === 'true';
    const customLimit = parseInt(req.query.limit) || null;
    
    // Optimized limits for better performance
    let actualLimit;
    if (customLimit) {
      actualLimit = customLimit;
    } else if (quickMode) {
      actualLimit = isAutoMode ? 10 : 15; // Even smaller for auto mode
    } else {
      actualLimit = maxJobs;
    }
    
    console.log(`üìä Mode: ${quickMode ? `QUICK (${actualLimit} regular jobs)` : 'FULL (all regular jobs)'}`);
    console.log(`ü§ñ Auto mode: ${isAutoMode ? 'YES' : 'NO'}`);
    
    // Clean old jobs from database periodically
    if (Math.random() < 0.1) { // 10% chance
      scrapedDB.cleanOldJobs();
    }
    
    browser = await puppeteer.launch({
      headless: process.env.PUPPETEER_HEADLESS !== 'false' ? 'new' : false,
      args: [
        '--no-sandbox',
        '--disable-setuid-sandbox',
        '--disable-dev-shm-usage',
        '--disable-blink-features=AutomationControlled',
        '--exclude-switches=enable-automation',
      ],
      ignoreDefaultArgs: ['--enable-automation'],
    });
    
    const page = await browser.newPage();
    await page.setUserAgent('Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36');
    
    // Step 1: Get regular job URLs with optimized pagination
    console.log('üîç Discovering regular job URLs (skipping sponsored)...');
    
    const allJobUrls = [];
    let currentPage = 1;
    const maxPages = quickMode ? 2 : 15; // Reduced pages for speed
    let totalSponsored = 0;
    let totalRegular = 0;
    let skippedAlreadyScraped = 0;
    
    while (currentPage <= maxPages) {
      try {
        // Generate correct URL based on pagination pattern
        let listingUrl;
        if (currentPage === 1) {
          listingUrl = 'https://www.expatriates.com/classifieds/saudi-arabia/jobs/';
        } else {
          const indexNumber = (currentPage - 1) * 100;
          listingUrl = `https://www.expatriates.com/classifieds/saudi-arabia/jobs/index${indexNumber}.html`;
        }
        
        console.log(`üìÑ Scanning page ${currentPage}: ${listingUrl}`);
        
        await page.goto(listingUrl, {
          waitUntil: 'networkidle0',
          timeout: 30000
        });
        
        await new Promise(resolve => setTimeout(resolve, 3000));
        
        // Extract job URLs and check if they're sponsored
        const pageResults = await page.evaluate(() => {
          const jobElements = Array.from(document.querySelectorAll('a'));
          const jobUrls = [];
          let sponsoredCount = 0;
          let regularCount = 0;
          
          jobElements.forEach(link => {
            const href = link.href;
            
            // Check if it's a valid job URL
            if (href && href.includes('/cls/') && href.includes('.html') && /\d{8}/.test(href)) {
              // Get the parent element text to check for sponsored vs regular pattern
              const parentText = link.parentElement ? link.parentElement.textContent.trim() : '';
              
              // Sponsored jobs: parent text ends with "Sponsored"
              // Regular jobs: parent text ends with timestamp pattern (e.g., "Tue, Jul 1, 2025, 5:43:12 PM - 3 minutes ago")
              const isSponsored = parentText.endsWith('Sponsored');
              const hasTimestamp = /\w{3}, \w{3} \d{1,2}, \d{4}, \d{1,2}:\d{2}:\d{2} [AP]M - (\d+ \w+ ago|an hour ago|a day ago)$/.test(parentText) ||
                                 /\d{1,2}\/\d{1,2}\/\d{4}|\d{1,2}-\d{1,2}-\d{4}|Today|Yesterday|ago|Posted/.test(parentText);
              
              if (isSponsored) {
                sponsoredCount++;
              } else if (hasTimestamp || (!isSponsored && parentText.length > 0)) {
                // This is a regular job - include it
                jobUrls.push(href);
                regularCount++;
              }
            }
          });
          
          return {
            jobUrls: [...new Set(jobUrls)], // Remove duplicates
            sponsoredCount,
            regularCount
          };
        });
        
        totalSponsored += pageResults.sponsoredCount;
        totalRegular += pageResults.regularCount;
        
        console.log(`üìä Page ${currentPage} results: ${pageResults.regularCount} regular jobs, ${pageResults.sponsoredCount} sponsored jobs`);
        
        // Add regular job URLs to our collection
        pageResults.jobUrls.forEach(url => {
          if (!allJobUrls.includes(url)) {
            allJobUrls.push(url);
          }
        });
        
        // If no jobs found on this page, we might have reached the end
        if (pageResults.jobUrls.length === 0 && pageResults.sponsoredCount === 0) {
          console.log(`üìã No jobs found on page ${currentPage}, stopping pagination`);
          break;
        }
        
        console.log(`üìã Total regular jobs found so far: ${allJobUrls.length}`);
        
        currentPage++;
        await new Promise(resolve => setTimeout(resolve, 2000)); // Rate limiting
        
      } catch (error) {
        console.log(`‚ö†Ô∏è Error on page ${currentPage}:`, error.message);
        break;
      }
    }
    
    console.log(`\\nüéØ DISCOVERY COMPLETE!`);
    console.log(`üìä Total URLs discovered: ${allJobUrls.length} regular jobs`);
    console.log(`üìä Sponsored jobs skipped: ${totalSponsored}`);
    console.log(`üìä Regular jobs found: ${totalRegular}`);
    
    if (allJobUrls.length === 0) {
      throw new Error('No regular job URLs found');
    }
    
    // Step 2: Filter out already scraped jobs and extract details
    console.log(`\nüîç Filtering out already scraped jobs...`);
    
    const newJobUrls = [];
    const alreadyScrapedUrls = [];
    
    // Quick pre-filtering based on posting ID in URL
    allJobUrls.forEach(url => {
      const postingIdMatch = url.match(/cls\/(\d+)\.html/);
      if (postingIdMatch) {
        const postingId = postingIdMatch[1];
        if (scrapedDB.hasJob(postingId)) {
          alreadyScrapedUrls.push(url);
          skippedAlreadyScraped++;
        } else {
          newJobUrls.push(url);
        }
      } else {
        newJobUrls.push(url); // Include if we can't extract posting ID
      }
    });
    
    console.log(`üìä Jobs analysis:`);
    console.log(`   üÜï New jobs to scrape: ${newJobUrls.length}`);
    console.log(`   ‚è≠Ô∏è Already scraped (skipped): ${skippedAlreadyScraped}`);
    
    const jobsToProcess = Math.min(actualLimit, newJobUrls.length);
    const scrapedJobs = [];
    const failedUrls = [];
    
    console.log(`\nüöÄ Starting to scrape ${jobsToProcess} NEW regular jobs...`);
    
    for (let i = 0; i < jobsToProcess; i++) {
      const jobUrl = newJobUrls[i];
      const progress = Math.round(((i + 1) / jobsToProcess) * 100);
      
      console.log(`\nüîç [${progress}%] Processing NEW job ${i + 1}/${jobsToProcess}`);
      console.log(`üìÑ ${jobUrl}`);
      
      const jobDetails = await extractJobFromUrl(page, jobUrl);
      
      if (jobDetails) {
        // Add to database to prevent future re-scraping
        if (jobDetails.posting_id) {
          scrapedDB.addJob(jobDetails.posting_id, {
            title: jobDetails.title,
            url: jobUrl,
            scraped_date: new Date().toISOString()
          });
        }
        
        scrapedJobs.push(jobDetails);
        console.log(`‚úÖ Extracted: ${jobDetails.title} | Salary: ${jobDetails.salary || 'N/A'} | Contact: ${jobDetails.contact || 'N/A'}`);
      } else {
        failedUrls.push(jobUrl);
        console.log(`‚ùå Failed to extract job details`);
      }
      
      // Faster rate limiting for auto mode
      const delay = isAutoMode ? 1500 : 2000;
      await new Promise(resolve => setTimeout(resolve, delay));
    }
    
    console.log(`\nüéâ OPTIMIZED SCRAPING COMPLETE! Extracted ${scrapedJobs.length} NEW regular jobs`);
    
    // Step 3: Save to CSV (only if we have new jobs)
    let csvResult = null;
    if (scrapedJobs.length > 0) {
      const filename = `regular_jobs_${quickMode ? 'quick_' : ''}${isAutoMode ? 'auto_' : ''}${new Date().toISOString().slice(0, 10)}.csv`;
      csvResult = saveJobsToCSV(scrapedJobs, filename);
      console.log(`üìä CSV saved: ${filename}`);
    }
    
    // Get database stats
    const dbStats = scrapedDB.getStats();
    
    // Calculate stats
    const successRate = jobsToProcess > 0 ? Math.round((scrapedJobs.length / jobsToProcess) * 100) : 0;
    
    const summary = {
      total_jobs: scrapedJobs.length,
      new_jobs: scrapedJobs.length,
      skipped_jobs: skippedAlreadyScraped,
      regular_jobs_only: true,
      sponsored_jobs_skipped: totalSponsored,
      with_salary: scrapedJobs.filter(job => job.salary).length,
      with_contact: scrapedJobs.filter(job => job.contact).length,
      with_email: scrapedJobs.filter(job => job.email).length,
      with_location: scrapedJobs.filter(job => job.location).length,
      with_description: scrapedJobs.filter(job => job.description).length,
      with_posted_date: scrapedJobs.filter(job => job.posted_date).length,
      extraction_method: 'regular_jobs_only_scraper'
    };
    
    res.json({
      success: true,
      message: `Successfully scraped ${scrapedJobs.length} NEW regular jobs (${skippedAlreadyScraped} already scraped, ${totalSponsored} sponsored skipped)`,
      stats: {
        totalRegularUrlsFound: allJobUrls.length,
        newJobsFound: newJobUrls.length,
        alreadyScrapedSkipped: skippedAlreadyScraped,
        sponsoredJobsSkipped: totalSponsored,
        regularJobsProcessed: jobsToProcess,
        regularJobsExtracted: scrapedJobs.length,
        failedExtractions: failedUrls.length,
        successRate: successRate,
        database_stats: dbStats
      },
      summary: summary,
      csvExport: csvResult,
      data: scrapedJobs,
      note: quickMode ? `Optimized quick mode: Limited to ${actualLimit} NEW regular jobs` : 'Full scrape of regular jobs completed',
      optimization: {
        duplicate_detection: 'Enabled - skips already scraped jobs using posting ID',
        auto_mode: isAutoMode,
        reduced_delays: isAutoMode ? 'Enabled for faster processing' : 'Disabled'
      },
      scraped_at: new Date().toISOString()
    });
    
  } catch (error) {
    console.error('‚ùå Regular jobs scraper error:', error);
    res.status(500).json({
      success: false,
      error: error.message,
      message: 'Failed to scrape regular jobs',
      suggestion: 'Try using ?quick=true for a smaller test batch'
    });
  } finally {
    if (browser) {
      try {
        await browser.close();
      } catch (e) {
        console.log('Browser close error:', e.message);
      }
    }
  }
};
